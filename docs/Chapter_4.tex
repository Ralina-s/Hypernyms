\section{Исследование и построение решений задачи}
\label{sec:Chapter_4} \index{Chapter_4}
\large




\subsection{Шаблонный метод}

\subsubsection{Извлечение из корпуса UMBC}

Для тестирования первого метода решения поставленной задачи, необходимо
было составить шаблоны поиска гипонимов и гиперонимов, аналогичных шаблонам
Марти Херст.

Были найдены ручные примеры шаблонов как в научных статьях, так и в
готовых скриптах, написанных на языке программирования. Наиболее широким
списком разнообразных шаблонов обладал класс \newline
\textit{hearstPatterns}, написанный на
языке Python. Всего в нем содержалось 48 примеров, написанных регулярными
выражениями.

Для удобной и более быстрой работы с данным классом, необходимо было
выгрузить исходный код и подправить его под текущую задачу. Производилась
внутренняя лемматизация слов и приведение их к нижнему регистру, а также
удаление пунктуации предложения. В итоге, была получена функция, на вход
которой передавалось необработанное предложение, а на выходе составлялся
список найденных пар гипоним-гипероним.

Целью проверки данного метода шаблонов было извлечение всех пар, связанных
отношением $is$-a, из текстового корпуса UMBC средствами класса \textit{hearstPattern}s, и
последующая оценка выбранными метриками.

Как оказалось на практике, написанных шаблонов было недостаточно для
качественного обнаружения искомых пар. В качестве примера: из 1,5 млн
предложений было обнаружено всего 8003 пары. Более того, на обработку одно
предложения всеми регулярными выражениями требовалось очень много времени.
Просмотр всего корпуса требовал более 5-ти дней. Поэтому добавление новых
рукописных шаблонов не представлялось возможным.

Результаты:

\begin{itemize}
\item MRR: 0.005
\item MAP: 0.0017
\item P@1: 0.0033
\item P@3: 0.0023
\item P@5: 0.0018
\item P@15: 0.0012
\end{itemize}

Не имея в наличии мощной техники, достаточного времени и богатого списка
шаблонов, протестировать данной метод в полном объеме не удалось.

\subsubsection{PROBASE}

Алгоритм, основанный на шаблонах, имеет высокую точность и не требует обучения.
Поэтому некоторые компании, обладающие большими ресурсами, составляют свои наборы
таких шаблонов и обрабатывают с их помощью огромное количество текстовых корпусов.
Одним из таких полученных наборов слов является ProBase компании Microsoft.

Probase представляет собой набор триплетов вида $(u, v, q)$, где $u$ — гипоним, $v$ —
гипероним, $q$ — количество раз, сколько пара $u-v$ встречалась в текстовых корпусах.
Microsoft обработал более 1.5 миллиарда веб страниц, содержащих текстовую
информацию различных областей, из которых удалось извлечь ~33 миллиона различных
триплетов.

На основе этих данных была построена модель, сопоставляющая каждому из 1500 гипонимов список гиперонимов, упорядоченных по уменьшению значения $q$, для соответствующей пары гипоним-гипероним. Например, если для слова \textit{море} в ProBase существуют триплеты: (\textit{море, отдых, 80}), (\textit{море, водоем, 100}) и (\textit{море, путешествие, 50}), и
других нет, то для этого слова составится список гиперонимов: [\textit{водоем, отдых,
путешествие}].

Протестировав данный алгоритм, были получены следующие результаты:


\begin{itemize}
\item MRR: 0.02649
\item MAP: 0.01239
\item P@1: 0.01343
\item P@3: 0.01119
\item P@5: 0.01166
\item P@15: 0.01305
\end{itemize}


В связи с просмотром большой базой текстовых корпусов, результаты этой модели ProBase оказались лучше, но все равно остаются совсем невысокими.



\subsection{PPMI + SVD}

Для исследования первого метода, основанного на дистрибутивной гипотезе, каждое
предложение текстового корпуса было разбито на контексты с шириной окна в 5 слов.
Составлена разреженная матрица частоты встречаемости пары (слово, контекст). Затем
полученная матрица была пересчитана в \textit{PPMI} матрицу по формулам, описанным в пункте
2.1.

Размер матрицы оказался слишком большим, чтобы было возможно применить алгоритм
снижения размерности \textit{SVD}. Были опробованы готовые реализации модели на языках
Python (классы numpy.linalg.svd и scipy.sparse.linalg.svds) и Matlab (svd), но ни одна из них не смогла преобразовать \textit{PPMI} матрицу.

Таким образом, метод \textit{PPMI} + \textit{SVD}, получения векторного представления слов,
протестировать на существующий данных не удалось.





\subsection{WORD2VEC}

\subsubsection{Модель GoogleNews}

Следующим исследуемым методом получения векторного представления слов был
Word2Vec.

Существует множество готовых обученных моделей, опубликованных в открытом доступе.
Для тестирования решения данной задачи была применена модель, имеющая архитектуру
CBOW и обученная на корпусе GoogleNews, размером в 3 миллиона слов. Основные
параметры: размерность вектора – 300, ширина окна – 5.

Алгоритм Word2Vec основывается на информации о контекстах, в которых употреблялось
слово в текстовом корпусе. Поэтому получение вектора возможно только для тех слов,
которые встречались в корпусе хотя бы 1 раз (в общем случае, не менее N раз, где порог N
является гиперпараметром). Значит, если гипоним, для которого необходимо найти все
гиперонимы, не встречался, в обучающем корпусе, то построенная модель не сможет
подобрать для него требуемый список.

Из 1500 гипонимов, находящихся в выбранном корпусе данных, построенная модель,
смогла предоставить вектора только для 77\%. Для оставшихся слов, необходимо было
применять другую модель.

Для получения итогового списка гиперонимов к каждому гипониму, было применено 2
различных алгоритма: \textit{GBR} (Gradient Boosting for regression) и \textit{LambdaRank}.

Алгоритм \textit{GBR} можно применить как pointwise алгоритм ранжирования. В то время как
LambdaRank является представителем pairwise подхода. Нельзя заранее сказать, какой из
двух подходов будет работать лучше, поэтому для тестирования были использованы оба.

\paragraph{Подготовка обучающего множества}
~\
~\

Так как необходимо было исследовать множество алгоритмов с различными параметрами,
для экономии скорости и минимальной потери точности, извлечение гиперонимов
происходило не из полного словаря, содержащего $\approx 220$ тыс слов. Для каждого гипонима
составлялся свой словарь по следующему алгоритму:

\begin{enumerate}
\item Добавлялись все слова из эталонного списка гиперонимов. Среднее количество
гиперонимов для каждого гипонима составляло 5 слов

\item Каждому слову из списка присваивалось значение, отражающее его позицию. Эти
величины служат целевым признаком для предсказания. Для алгоритма
LambdaRank чаще всего используются значения 0,1,2 и 3, поэтому для эталонных
гиперонимов были выбраны значения 1,2 и 3. Список делился на три части, если
слово находилось в первой из них, ему присваивалась величина 3, если во
второй, то 2, оставшейся последней части - 1.

\item Далее в случайном порядке добавлялись к полученному списку дополнительно 500
слов, играющих роль негативных примеров. Каждое такое слово имело целевое
значение 0.
\end{enumerate}

В качестве признаков для обучения моделей были протестированы следующие
комбинации векторов гипонима и гиперонима:

\begin{enumerate}
\item Разность векторов: \newline
$Diff: <u - v>$

\item Конкатенация векторов + евклидово расстояние степени 1: \newline
$||u - v||_1 = \sum_{i=1}^{d}|u_i - v_i|$ \newline
$Dist1: <u, v, ||u - v||_1>$

\item Конкатенация векторов + евклидово расстояние степени 2: \newline
$||u - v||_2 = \sqrt{\sum_{i=1}^{d}|(u_i - v_i)^2|}$ \newline
$Dist2: <u, v, || u - v ||_2>$

\item Конкатенация векторов + косинусное расстояние между ними: \newline
$Cos: <u, v, cos(u, v)>$
\end{enumerate}

Полученный новый набор данных разделялся на обучающую и тестовую выборку в
отношении 2 : 1. Так как не для всех гипонимов построены вектора, то тестирование и
обучение происходило только на 77\% данных.

\paragraph{Тестирование}
~\
~\

Получены следующие результаты:


\begin{table}[!htb]

\begin{minipage}{.5\textwidth}
\centering
%--------------------------------------
\begin{tabular}{|l|l|l|}
\hline
 & \textbf{MRR} & \textbf{MAP} \\

\hline
\textbf{Diff} & $0.914$ & $0.504$\\

\hline
\textbf{Dist1} & $0.942$ & $0.487$\\

\hline
\textbf{Dist2} & $0.914$ & $0.787$\\

\hline
\textbf{Cos} & $0.039$ & $0.044$\\

\hline
\end{tabular}
%--------------------------------------
\caption{LambdaRank}
\label{tabular:LambdaRank}
\end{minipage}%
\begin{minipage}{.5\textwidth}
\centering
%--------------------------------------
\begin{tabular}{|l|l|l|}
\hline
 & \textbf{MRR} & \textbf{MAP} \\

\hline
\textbf{Diff} & $0.893$ & $0.442$\\

\hline
\textbf{Dist1} & $0.912$ & $0.463$\\

\hline
\textbf{Dist2} & $0.918$ & $0.659$\\

\hline
\textbf{Cos} & $0.078$ & $0.062$\\

\hline
\end{tabular}
%--------------------------------------
\caption{GBR}
\label{tabular:GBR}
\end{minipage}

\end{table}


Лучший результат среди всех опробованных моделей с векторами \textit{Word2Vec} - GoogleNews,
показал алгоритм \textit{LambdaRank} с основными параметрами: шаг обучения = 0.1, кол-во
деревьев 100, доля выборки на каждом шаге обучения (subsample) = 0.8, максимальная
глубина 5. Для метрики \textit{MRR}, более успешными было представление векторов
комбинацией Dist1, а для \textit{MAP} - Dist2.

\subsubsection{Обучение собственное модели WORD2VEC}

Далее, был обучен алгоритм \textit{Word2Vec} на текстовом корпусе UBMC. Целью такого
исследования было увеличение доли гипонимов, для которых возможно построить вектор,
и возможность настроить главные гиперпараметры.

Из опробованных комбинаций параметров, лучший результат показала модель Skip-gramm,
с шириной окна 7 и размером вектора 300.

Так как среди гипонимов встречались не только слова, но и словосочетания из 2-3 слов, не
удалось построить вектора для них всех. Доля с 77\% увеличилась до 82\%.

\paragraph{Тестирование}
~\
~\

Были применены все те подходы, которые использовались для эксперимента с моделью
\textit{Word2Vec} - GoogleNews.

Лучшей также оказалась модель \textit{LambdaRank} с теми же параметрами.

Получены следующие результаты:

\begin{table}[!htb]

\begin{minipage}{.5\textwidth}
\centering
%--------------------------------------
\begin{tabular}{|l|l|l|}
\hline
 & \textbf{MRR} & \textbf{MAP} \\

\hline
\textbf{Diff} & $0.945$ & $0.489$\\

\hline
\textbf{Dist1} & $0.931$ & $0.705$\\

\hline
\textbf{Dist2} & $0.924$ & $0.793$\\

\hline
\textbf{Cos} & $0.035$ & $0.020$\\

\hline
\end{tabular}
%--------------------------------------
\caption{LambdaRank}
\label{tabular:LambdaRank2}
\end{minipage}%
\begin{minipage}{.5\textwidth}
\centering
%--------------------------------------
\begin{tabular}{|l|l|l|}
\hline
 & \textbf{MRR} & \textbf{MAP} \\

\hline
\textbf{Diff} & $0.914$ & $0.459$\\

\hline
\textbf{Dist1} & $0.921$ & $0.588$\\

\hline
\textbf{Dist2} & $0.927$ & $0.655$\\

\hline
\textbf{Cos} & $0.081$ & $0.073$\\

\hline
\end{tabular}
%--------------------------------------
\caption{GBR}
\label{tabular:GBR2}
\end{minipage}

\end{table}



\subsection{DYNAMIC DISTANCE-MARGIN MODEL}

Для применения данного алгоритма, необходимо иметь набор триплетов $(u, v, q)$, где $u$ - гипоним, $v$ - гипероним, а $q$ - сколько раз пара гипоним-гипероним $(u, v)$ встретилась в текстовом корпусе. В качестве такого набора данных был взят ProBase, описанный в главе
шаблонных методов.

ProBase был получен на основе очень больших наборов текстовых корпусов, поэтому
значения q могли достигать величины в 35000. Алгоритм \textit{DDM} (Dynamic distance-margin),
учитывает и обучает каждую пару столько раз, сколько она встречалась. Таким образом,
некоторые пары могли учитываться 35 тысяч раз за одну эпоху, в то время, как
большинство других имели значение $q < 20$. В таком случае модель могла практически не
обучить большинство векторов. Чтобы устранить такой большой разрыв, значение q было
изменено по формуле $\sqrt[1.85]{q}$. Степень корня подбиралась так, чтобы
максимальное значение не было слишком большим или слишком маленьким.

Для каждого триплета $x$ подбирался негативный триплет $x'$, где был заменен либо
гипероним, либо гипоним на случайный. Чтобы детерминировать данный выбор и
не вносить различия в обучение, было решено для каждой пары подбирать сразу 2
негативных триплета - негативный гипоним и негативный гипероним.

Нейронная сеть, используемая в данной модели, обучает входные вектора. Поэтому, было
решено вручную рассчитать градиенты для изменения этих векторов (метод обратного
распространения ошибки).

После расчетов, получился следующий алгоритм изменения векторов на каждой эпохе:


\begin{algorithmic}

\For{$x = (u, v, q)$ in $X$}
\Comment{для каждой пары из ProBase}
	\For{$i$ in $[1 . . q]$}
    \Comment{для каждой пары негативных примеров}
        \State $x'_i$ = (u, $v'_i$, $q'_i$)
        \Comment{негативный пример гиперонима}
        \If {$f(x)$ $+$ $log(q)$ $<$ $f(x'_i)$ $+$ $log(q'_i)$}
        	\State $u += (u - v)$ / $|| u - v ||_2$
            \State $v -= (u - v)$ / $|| u - v ||_2$
            \State $u -= (u - v')$ / $|| u - v' ||_2$
            \State $v' += (u - v')$ / $|| u - v' ||_2$
    	\EndIf
        \State
        \State $x'_i$ = ($u'_i$, v, $q'_i$)
        \Comment{негативный пример гипонима}
        \If {$f(x)$ $+$ $log(q)$ $<$ $f(x'_i)$ $+$ $log(q'_i)$}
        	\State $u += (u - v)$ / $|| u - v ||_2$
            \State $v -= (u - v)$ / $|| u - v ||_2$
            \State $u' -= (u' - v)$ / $|| u' - v ||_2$
            \State $v += (u' - v)$ / $|| u' - v ||_2$
    	\EndIf
    \EndFor
\EndFor
\end{algorithmic}

Для одной эпохи необходимо было рассчитать расстояния и градиент для более 170 миллионов пар слов.

Был реализован алгоритм на языке Python, но как и следовало ожидать, для его обучения
не было достаточно памяти и разумного времени.

После исследования данных ProBase, было установлено, что не со всеми словами из этого
набора, связаны необходимые для нашей задачи гипонимы. Была выделена отдельная
компонента связности, составляющая 1 / 15 часть всего набора пар ProBase. То есть
обучение 14 / 15 всех пар векторов никак не влияло на улучшение результата
поставленной задачи.

После уменьшение данных в 15 раз была проведена еще одна попытка запуска алгоритма,
написанного на языке Python. Уменьшение данных все равно не позволило обучить
алгоритм, что привело к поиску других решений реализации данной модели.

\subsubsection{Распараллеливание алгоритма средствами HADOOP}

Одним из используемых современных решений работы с большим объемом данных
является парадигма параллельных вычислений Hadoop MapReduce. Хорошо иллюстрирует
данный подход схема, расположенная ниже.

\begin{figure}[H]
\centering 
    \includegraphics[scale=0.6]{image/MapRed.png}
    \caption{Схема распараллеливание.}
    \label{srg}
\end{figure}

На стадии Map параллельно считываются данные из независимых блоков, на которые
разбит исходный входной файл. Происходит их преобразование в пары (Ключ, Значение).
Затем идет стадия Shuffle: вывод функции Map распределяется по «корзинам», в
зависимости от значения ключа. Все пары (Ключ, Значение), имеющие одинаковый ключ,
попадают в одну корзину и отдаются определенному редьюсеру. Таким образом, данные
из разных частей входного файла могут собраться вместе при обработке на стадии
Reduce. Такая архитектура позволяет быстрое распараллеливание и не загружает
оперативную память, что необходимо при решении поставленной задачи.

Краткая реализация алгоритма \textit{DDM} в парадигме \textit{MapReduce}:

\begin{enumerate}
\item Считываются значения векторов для каждого слова. Для первой эпохи значения
заполняются случайными величинами в диапазоне $[-0.1, 0.1]$.

\item На стадии Map происходит параллельное считывание пар $(u, v, q)$.

\item Формируется пара (Ключ, Значение) для её обработки на стадии Reduce. Ключ = $u$,
Значение = $v$. Отправляются на Reduce.

\item Для каждой такой пары генерируется $q$ негативных примеров гипонимов $u'$ и $q$
негативных примеров гиперонимов $v'$.

\item Для каждой пары $(u, v')$ ( аналогично для $(u', v)$) рассчитывается разница расстояния между $u$ и $v'$ и расстояния между $u$, $v$. Если $f(x) + \log(q) < f(x') + \log(q')$, то необходимо изменить вектора $u, v, v'$ - переход к пункту 4. Иначе, просматривается следующая
пара.

\item Рассчитываются антиградиенты $du, dv, dv'$ и умножаются на шаг обучения.

\item Формируются пары (Ключ, Значение) для их обработки на стадии Reduce. Ключ
соответствует самому вектору, а значение - изменению. Получаются пары $(u, du)$, $(v,
dv)$, $(v’, dv’)$. Отправляются на Reduce.

\item На стадии Reduce для каждого вектора $u$ (аналогично $v$), собирается список всех
его изменений, полученных из пункта 6 и изначальное значение вектора из пункта 1.

\item Высчитывается новое значение вектора и сохраняется в файле, для его считывания
следующей эпохой.
\end{enumerate}


\subsubsection{Тестирование}

При распараллеливании данного алгоритма на 24 кластера, получилось, что средняя
продолжительность вычисления одной эпохи составляет 50 секунд.

Параметры сети были установлены следующими: длина вектора 100, шаг обучения 0.1, количество эпох 100.

Были получены вектора для 91.5\% гипонимов, что показало наибольшее покрытие среди
всех реализованных в данной работе методов.

Также, как и при исследовании метода \textit{Word2Vec} были протестированы алгоритмы \textit{GBR} и \textit{LambdaRank} с видами представления векторов: $Diff$,\newline $Dist1, Dist2, Cos$.

Полученные результаты:
\begin{table}[!htb]

\begin{minipage}{.5\textwidth}
\centering
%--------------------------------------
\begin{tabular}{|l|l|l|}
\hline
 & \textbf{MRR} & \textbf{MAP} \\

\hline
\textbf{Diff} & $0.922$ & $0.739$\\

\hline
\textbf{Dist1} & $0.943$ & $0.847$\\

\hline
\textbf{Dist2} & $0.954$ & $0.874$\\

\hline
\textbf{Cos} & $0.101$ & $0.083$\\

\hline
\end{tabular}
%--------------------------------------
\caption{LambdaRank}
\label{tabular:LambdaRank2}
\end{minipage}%
\begin{minipage}{.5\textwidth}
\centering
%--------------------------------------
\begin{tabular}{|l|l|l|}
\hline
 & \textbf{MRR} & \textbf{MAP} \\

\hline
\textbf{Diff} & $0.922$ & $0.647$\\

\hline
\textbf{Dist1} & $0.937$ & $0.758$\\

\hline
\textbf{Dist2} & $0.915$ & $0.744$\\

\hline
\textbf{Cos} & $0.078$ & $0.015$\\

\hline
\end{tabular}
%--------------------------------------
\caption{GBR}
\label{tabular:GBR2}
\end{minipage}

\end{table}

Данная модель показала самый лучший результат среди всех реализованных в данной работе.