{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy import sparse\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "from operator import itemgetter\n",
    "import os\n",
    "import random\n",
    "import subprocess\n",
    "import pickle\n",
    "import re\n",
    "from time import time\n",
    "\n",
    "from numpy.linalg import svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "reload(logging)\n",
    "logging.basicConfig(format=\"%(asctime)s   %(message)s\", level=logging.INFO, datefmt=\"%H:%M:%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Просмотр данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_type = {}\n",
    "concepts = []\n",
    "entities = []\n",
    "with open(\"SemEval18-Task9/training/data/1A.english.training.data.txt\") as trainF:\n",
    "    for line in trainF.readlines():\n",
    "        infa = line.strip().split('\\t')\n",
    "        word = infa[0].lower()\n",
    "        type_word = infa[1]\n",
    "        \n",
    "        if type_word == 'Concept':\n",
    "            concepts.append(word)\n",
    "        else:\n",
    "            entities.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<< CONCEPTS >>\n",
      "\n",
      "blackfly\n",
      "abhorrence\n",
      "tropical storm\n",
      "militarization\n",
      "pollution\n",
      "photomicrograph\n",
      "swamp gum\n",
      "song\n",
      "wing\n",
      "cumulus\n",
      "silver\n",
      "sand verbena\n",
      "louse\n",
      "sodium nitrite\n",
      "navigator\n",
      "gasworks\n",
      "farming\n",
      "failure rate\n",
      "chess\n",
      "floorshow\n",
      "...\n",
      "\n",
      "All: 979\n"
     ]
    }
   ],
   "source": [
    "print \"<< CONCEPTS >>\"\n",
    "print \n",
    "\n",
    "\n",
    "conc = 0\n",
    "for w in concepts:\n",
    "    conc += 1\n",
    "    if conc <= 20:\n",
    "        print w\n",
    "    \n",
    "        \n",
    "print \"...\"\n",
    "print \n",
    "print \"All: \" + str(conc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<< ENTITIES >>\n",
      "\n",
      "turonian\n",
      "hero\n",
      "carly fiorina\n",
      "erna brodber\n",
      "raoul dufy\n",
      "skara brae\n",
      "murder one\n",
      "emmett tyrrell\n",
      "kim il-sung university\n",
      "sachs harbour\n",
      "myrdal\n",
      "crowds and power\n",
      "a1 motorway\n",
      "muhammad ali jinnah\n",
      "bennington\n",
      "helen of troy\n",
      "north carolina\n",
      "forest way\n",
      "younger dryas\n",
      "new zealand\n",
      "...\n",
      "\n",
      "All: 521\n"
     ]
    }
   ],
   "source": [
    "print \"<< ENTITIES >>\"\n",
    "print \n",
    "\n",
    "\n",
    "ent = 0\n",
    "for w in entities:\n",
    "    ent += 1\n",
    "    if ent <= 20:\n",
    "        print w\n",
    "    \n",
    "        \n",
    "print \"...\"\n",
    "print \n",
    "print \"All: \" + str(ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hipon_type = []\n",
    "with open(\"SemEval18-Task9/training/data/1A.english.training.data.txt\") as trainF:\n",
    "    for line in trainF.readlines():\n",
    "        infa = line.strip().split('\\t')\n",
    "        word = infa[0].lower()\n",
    "        type_word = infa[1]\n",
    "        hipon_type.append([word, type_word])\n",
    "hipon_type = np.array(hipon_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hipons = np.array(hipon_type)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hipon_test_type = []\n",
    "with open(\"SemEval18-Task9/trial/data/1A.english.trial.data.txt\") as trainF:\n",
    "    for line in trainF.readlines():\n",
    "        infa = line.strip().split('\\t')\n",
    "        word = infa[0].lower()\n",
    "        type_word = infa[1]\n",
    "        hipon_test_type.append([word, type_word])\n",
    "hipon_test_type = np.array(hipon_test_type)\n",
    "\n",
    "hipons_test = np.array(hipon_test_type)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hipers = []\n",
    "with open(\"SemEval18-Task9/training/gold/1A.english.training.gold.txt\") as trainF:\n",
    "    for line in trainF.readlines():\n",
    "        words = map(lambda w: w.lower(), line.strip().split('\\t'))\n",
    "        hipers.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<< Примеры гипоним: гиперонимы >>\n",
      "\n",
      "blackfly: homopterous insect, insect\n",
      "\n",
      "turonian: technical specification, geologic timescale, physical property, geological period, magnitude, unit of time, geological time, geologic time\n",
      "\n",
      "abhorrence: distaste, hatred, hate, disgust\n",
      "\n",
      "tropical storm: atmosphere, windstorm, violent storm, air current, atmospheric state, density, current of air, storm damage, atmospheric phenomenon, storm, cyclone, natural phenomenon, tempest, wind\n",
      "\n",
      "militarization: social control\n",
      "\n",
      "pollution: dirtiness, dirtying, environmental condition, impurity, sanitary condition, uncleanness\n",
      "\n",
      "photomicrograph: picture, photograph, photo, image, pic, digital image\n",
      "\n",
      "swamp gum: eucalyptus, plant, eucalyptus tree, angiosperm, gum, gum tree, woody plant, eucalypt\n",
      "\n",
      "song: possession, sound, work of art, musical composition, vocal music, musical work, piece of music, human language, speech, signaling\n",
      "\n",
      "wing: emblem, technical specification, form, specifications, symbolization, aerofoil, air unit, helping, shape, symbolisation, animal, airfoil\n",
      "\n",
      "hero: show, series, piece of music, title of respect, imaginary being, video, physical phenomenon, film, periodical publication, story, musical work, personal name, tv station, tv show, transport, mythical being, tv series, literary composition, affix, transportation, mass media, periodical literature, adventure story, passenger transport, honorific, television show, noun, bollywood, novel, tv program, film genre, fictitious character, documentary, philology, television program, drama, person, broadcaster, venture, movie, visual art, motion picture, video game, expression, mathematical relation\n",
      "\n",
      "...\n",
      "\n",
      "All: 1500\n"
     ]
    }
   ],
   "source": [
    "print \"<< Примеры гипоним: гиперонимы >>\"\n",
    "print\n",
    "\n",
    "i = 0 \n",
    "for O, E in zip(hipon_type[:,0], hipers):\n",
    "    line = O + ': '\n",
    "    for a in E[:-1]:\n",
    "        line += a + ', '\n",
    "    line += E[-1]\n",
    "    print line\n",
    "    print\n",
    "    i += 1\n",
    "    if i > 10:\n",
    "        break\n",
    "        \n",
    "print \"...\"\n",
    "print \n",
    "print \"All: \" + str(len(hipers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_nE = {}\n",
    "nE_vocab = {}\n",
    "with open(\"SemEval18-Task9/vocabulary/1A.english.vocabulary.txt\") as vocF:\n",
    "    for i, line in enumerate(vocF.readlines()):\n",
    "        word = line.strip().lower()\n",
    "        vocab_nE[word] = i\n",
    "        nE_vocab[i] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Кол-во слов(фраз) в словаре: 218755\n"
     ]
    }
   ],
   "source": [
    "print \"Кол-во слов(фраз) в словаре: \" + str(len(vocab_nE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(train, pred):\n",
    "    return subprocess.check_output(\"python ./SemEval18-Task9/task9-scorer.py \" + train + \" \" + pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patterns Hearest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://github.com/mmichelsonIF/hearst_patterns_python/blob/master/\n",
    "\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tag.perceptron import PerceptronTagger\n",
    "\n",
    "class HearstPatterns(object):\n",
    "\n",
    "    def __init__(self, extended = False):\n",
    "        self.__chunk_patterns = r\"\"\" #  helps us find noun phrase chunks\n",
    "                NP: {<DT|PP\\$>?<JJ>*<NN>+}\n",
    "                    {<NNP>+}\n",
    "                    {<NNS>+}\n",
    "        \"\"\"\n",
    "\n",
    "        self.__np_chunker = nltk.RegexpParser(self.__chunk_patterns) # create a chunk parser \n",
    "\n",
    "        # now define the Hearst patterns\n",
    "        # format is <hearst-pattern>, <general-term>\n",
    "        # so, what this means is that if you apply the first pattern, the firsr Noun Phrase (NP)\n",
    "        # is the general one, and the rest are specific NPs\n",
    "        self.__hearst_patterns = [\n",
    "                (\"(NP_\\w+ (, )?such as (NP_\\w+ ? (, )?(and |or )?)+)\", \"first\"),\n",
    "                (\"(such NP_\\w+ (, )?as (NP_\\w+ ?(, )?(and |or )?)+)\", \"first\"),\n",
    "                (\"((NP_\\w+ ?(, )?)+(and |or )?other NP_\\w+)\", \"last\"),\n",
    "                (\"(NP_\\w+ (, )?including (NP_\\w+ ?(, )?(and |or )?)+)\", \"first\"),\n",
    "                (\"(NP_\\w+ (, )?especially (NP_\\w+ ?(, )?(and |or )?)+)\", \"first\"),\n",
    "            ]\n",
    "\n",
    "        if extended:\n",
    "            self.__hearst_patterns.extend([\n",
    "                (\"((NP_\\w+ ?(, )?)+(and |or )?any other NP_\\w+)\", \"last\"),\n",
    "                (\"((NP_\\w+ ?(, )?)+(and |or )?some other NP_\\w+)\", \"last\"),\n",
    "                (\"((NP_\\w+ ?(, )?)+(and |or )?is a NP_\\w+)\", \"last\"),\n",
    "                (\"((NP_\\w+ ?(, )?)+(and |or )?was a NP_\\w+)\", \"last\"),\n",
    "                (\"((NP_\\w+ ?(, )?)+(and |or )?were a NP_\\w+)\", \"last\"),\n",
    "                (\"((NP_\\w+ ?(, )?)+(and |or )?are a NP_\\w+)\", \"last\"),\n",
    "                (\"(NP_\\w+ (, )?like (NP_\\w+ ? (, )?(and |or )?)+)\", \"first\"),\n",
    "                (\"such (NP_\\w+ (, )?as (NP_\\w+ ? (, )?(and |or )?)+)\", \"first\"),\n",
    "                (\"((NP_\\w+ ?(, )?)+(and |or )?like other NP_\\w+)\", \"last\"),\n",
    "                (\"((NP_\\w+ ?(, )?)+(and |or )?one of the NP_\\w+)\", \"last\"),\n",
    "                (\"((NP_\\w+ ?(, )?)+(and |or )?one of these NP_\\w+)\", \"last\"),\n",
    "                (\"((NP_\\w+ ?(, )?)+(and |or )?one of those NP_\\w+)\", \"last\"),\n",
    "                (\"examples of (NP_\\w+ (, )?is (NP_\\w+ ? (, )?(and |or )?)+)\", \"first\"),\n",
    "                (\"examples of (NP_\\w+ (, )?are (NP_\\w+ ? (, )?(and |or )?)+)\", \"first\"),\n",
    "                (\"((NP_\\w+ ?(, )?)+(and |or )?are examples of NP_\\w+)\", \"last\"),\n",
    "                (\"((NP_\\w+ ?(, )?)+(and |or )?is example of NP_\\w+)\", \"last\"),\n",
    "                (\"(NP_\\w+ (, )?for example (NP_\\w+ ? (, )?(and |or )?)+)\", \"first\"),\n",
    "                (\"((NP_\\w+ ?(, )?)+(and |or )?wich is called NP_\\w+)\", \"last\"),\n",
    "                (\"((NP_\\w+ ?(, )?)+(and |or )?which is named NP_\\w+)\", \"last\"),\n",
    "                (\"(NP_\\w+ (, )?mainly (NP_\\w+ ? (, )?(and |or )?)+)\", \"first\"),\n",
    "                (\"(NP_\\w+ (, )?mostly (NP_\\w+ ? (, )?(and |or )?)+)\", \"first\"),\n",
    "                (\"(NP_\\w+ (, )?notably (NP_\\w+ ? (, )?(and |or )?)+)\", \"first\"),\n",
    "                (\"(NP_\\w+ (, )?particularly (NP_\\w+ ? (, )?(and |or )?)+)\", \"first\"),\n",
    "                (\"(NP_\\w+ (, )?principally (NP_\\w+ ? (, )?(and |or )?)+)\", \"first\"),\n",
    "                (\"(NP_\\w+ (, )?in particular (NP_\\w+ ? (, )?(and |or )?)+)\", \"first\"),\n",
    "                (\"(NP_\\w+ (, )?except (NP_\\w+ ? (, )?(and |or )?)+)\", \"first\"),\n",
    "                (\"(NP_\\w+ (, )?other than (NP_\\w+ ? (, )?(and |or )?)+)\", \"first\"),\n",
    "                (\"(NP_\\w+ (, )?e.g. (NP_\\w+ ? (, )?(and |or )?)+)\", \"first\"),\n",
    "                (\"(NP_\\w+ (, )?i.e. (NP_\\w+ ? (, )?(and |or )?)+)\", \"first\"),\n",
    "                (\"((NP_\\w+ ?(, )?)+(and |or )?a kind of NP_\\w+)\", \"last\"),\n",
    "                (\"((NP_\\w+ ?(, )?)+(and |or )?kinds of NP_\\w+)\", \"last\"),\n",
    "                (\"((NP_\\w+ ?(, )?)+(and |or )?form of NP_\\w+)\", \"last\"),\n",
    "                (\"((NP_\\w+ ?(, )?)+(and |or )?forms of NP_\\w+)\", \"last\"),\n",
    "                (\"((NP_\\w+ ?(, )?)+(and |or )?which looks like NP_\\w+)\", \"last\"),\n",
    "                (\"((NP_\\w+ ?(, )?)+(and |or )?which sounds like NP_\\w+)\", \"last\"),\n",
    "                (\"(NP_\\w+ (, )?which are similar to (NP_\\w+ ? (, )?(and |or )?)+)\", \"first\"),\n",
    "                (\"(NP_\\w+ (, )?which is similar to (NP_\\w+ ? (, )?(and |or )?)+)\", \"first\"),\n",
    "                (\"(NP_\\w+ (, )?examples of this is (NP_\\w+ ? (, )?(and |or )?)+)\", \"first\"),\n",
    "                (\"(NP_\\w+ (, )?examples of this are (NP_\\w+ ? (, )?(and |or )?)+)\", \"first\"),\n",
    "                (\"(NP_\\w+ (, )?types (NP_\\w+ ? (, )?(and |or )?)+)\", \"first\"),\n",
    "                (\"((NP_\\w+ ?(, )?)+(and |or )? NP_\\w+ types)\", \"last\"),\n",
    "                (\"(NP_\\w+ (, )?whether (NP_\\w+ ? (, )?(and |or )?)+)\", \"first\"),\n",
    "                (\"(compare (NP_\\w+ ?(, )?)+(and |or )?with NP_\\w+)\", \"last\"),\n",
    "                (\"(NP_\\w+ (, )?compared to (NP_\\w+ ? (, )?(and |or )?)+)\", \"first\"),\n",
    "                (\"(NP_\\w+ (, )?among them (NP_\\w+ ? (, )?(and |or )?)+)\", \"first\"),\n",
    "                (\"((NP_\\w+ ?(, )?)+(and |or )?as NP_\\w+)\", \"last\"),\n",
    "                (\"(NP_\\w+ (, )? (NP_\\w+ ? (, )?(and |or )?)+ for instance)\", \"first\"),\n",
    "                (\"((NP_\\w+ ?(, )?)+(and |or )?sort of NP_\\w+)\", \"last\"),\n",
    "            ])\n",
    "\n",
    "        self.__pos_tagger = PerceptronTagger()\n",
    "        \n",
    "    def prepare(self, rawtext):\n",
    "        try:\n",
    "            sentences = nltk.sent_tokenize(rawtext.strip()) # NLTK default sentence segmenter\n",
    "            sentences = [nltk.word_tokenize(sent) for sent in sentences] # NLTK word tokenizer\n",
    "            sentences = [self.__pos_tagger.tag(sent) for sent in sentences] # NLTK POS tagger\n",
    "        except:\n",
    "            sentences = []\n",
    "        return sentences\n",
    "\n",
    "    def chunk(self, rawtext):\n",
    "        sentences = self.prepare(rawtext.strip())\n",
    "\n",
    "        all_chunks = []\n",
    "        for sentence in sentences:\n",
    "            chunks = self.__np_chunker.parse(sentence) # parse the example sentence\n",
    "            #for chunk in chunks:\n",
    "            #   print(str(chunk))\n",
    "            all_chunks.append(self.prepare_chunks(chunks))\n",
    "        return all_chunks\n",
    "\n",
    "    def prepare_chunks(self, chunks):\n",
    "        # basically, if the chunk is NP, keep it as a string taht starts w/ NP and replace \" \" with _\n",
    "        # otherwise, keep the word.\n",
    "        # remove punct\n",
    "        # this is all done to make it super easy to apply the Hearst patterns...\n",
    "\n",
    "        terms = []\n",
    "        for chunk in chunks:\n",
    "            label = None\n",
    "            try: # gross hack to see if the chunk is simply a word or a NP, as we want. But non-NP fail on this method call\n",
    "                label = chunk.label()\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            if label is None: #means one word...\n",
    "                token = chunk[0]\n",
    "                pos = chunk[1]\n",
    "                if pos in ['.', ':', '-', '_']:\n",
    "                    continue\n",
    "                terms.append(token)\n",
    "            else:\n",
    "                np = \"NP_\"+\"_\".join([a[0] for a in chunk]) #This makes it easy to apply the Hearst patterns later\n",
    "                terms.append(np)\n",
    "        return ' '.join(terms)\n",
    "\n",
    "    \"\"\"\n",
    "        This is the main entry point for this code.\n",
    "        It takes as input the rawtext to process and returns a list of tuples (specific-term, general-term)\n",
    "        where each tuple represents a hypernym pair.\n",
    "\n",
    "    \"\"\"\n",
    "    def find_hyponyms(self, rawtext):\n",
    "\n",
    "        hyponyms = []\n",
    "        np_tagged_sentences = self.chunk(rawtext)\n",
    "\n",
    "        for raw_sentence in np_tagged_sentences:\n",
    "            # two or more NPs next to each other should be merged into a single NP, it's a chunk error\n",
    "\n",
    "            # find any N consecutive NP_ and merge them into one...\n",
    "            # So, something like: \"NP_foo NP_bar blah blah\" becomes \"NP_foo_bar blah blah\"\n",
    "            sentence = re.sub(r\"(NP_\\w+ NP_\\w+)+\", lambda m: m.expand(r'\\1').replace(\" NP_\", \"_\"), raw_sentence)\n",
    "\n",
    "            for (hearst_pattern, parser) in self.__hearst_patterns:\n",
    "                matches = re.search(hearst_pattern, sentence)\n",
    "                if matches:\n",
    "                    match_str = matches.group(0)\n",
    "\n",
    "                    nps = [a for a in match_str.split() if a.startswith(\"NP_\")]\n",
    "\n",
    "                    if parser == \"first\":\n",
    "                        general = nps[0]\n",
    "                        specifics = nps[1:]\n",
    "                    else:\n",
    "                        general = nps[-1]\n",
    "                        specifics = nps[:-1]\n",
    "                        #print(str(general))\n",
    "                        #print(str(nps))\n",
    "\n",
    "                    for i in range(len(specifics)):\n",
    "                        #print(\"%s, %s\" % (specifics[i], general))\n",
    "                        hyponyms.append((self.clean_hyponym_term(specifics[i]), self.clean_hyponym_term(general)))\n",
    "\n",
    "        return hyponyms\n",
    "\n",
    "\n",
    "    def clean_hyponym_term(self, term):\n",
    "        # good point to do the stemming or lemmatization\n",
    "        return term.replace(\"NP_\",\"\").replace(\"_\", \" \")\n",
    "\n",
    "hearst_patterns = HearstPatterns()\n",
    "hearst_patterns.find_hyponyms(\"There are works by such authors as Herrick, Goldsmith, and Shakespeare.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_O(ac):\n",
    "    groups = {}\n",
    "    for v, O, E in zip(ac.data, ac.row, ac.col):\n",
    "        groups.setdefault(O, [])\n",
    "        groups[O].append([E, v])\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21:49:27   line: 30000, found_pairs 1145, get_pairs 271\n",
      "21:50:44   line: 60000, found_pairs 1735, get_pairs 397\n",
      "21:52:08   line: 90000, found_pairs 1973, get_pairs 448\n",
      "21:53:31   line: 120000, found_pairs 2256, get_pairs 525\n",
      "21:54:56   line: 150000, found_pairs 2671, get_pairs 638\n",
      "21:56:16   line: 180000, found_pairs 3399, get_pairs 793\n",
      "21:57:32   line: 210000, found_pairs 4343, get_pairs 979\n",
      "21:58:47   line: 240000, found_pairs 5301, get_pairs 1201\n",
      "22:00:07   line: 270000, found_pairs 6570, get_pairs 1414\n",
      "22:01:20   line: 300000, found_pairs 7293, get_pairs 1522\n",
      "22:02:35   line: 330000, found_pairs 8048, get_pairs 1687\n",
      "22:03:55   line: 360000, found_pairs 9281, get_pairs 1929\n",
      "22:05:19   line: 390000, found_pairs 10481, get_pairs 2195\n",
      "22:06:40   line: 420000, found_pairs 11534, get_pairs 2368\n",
      "22:08:06   line: 450000, found_pairs 12532, get_pairs 2562\n",
      "22:09:11   line: 480000, found_pairs 12867, get_pairs 2638\n",
      "22:10:23   line: 510000, found_pairs 13316, get_pairs 2753\n",
      "22:11:30   line: 540000, found_pairs 13751, get_pairs 2851\n",
      "22:12:53   line: 570000, found_pairs 14967, get_pairs 3101\n",
      "22:14:15   line: 600000, found_pairs 15723, get_pairs 3260\n",
      "22:15:31   line: 630000, found_pairs 16891, get_pairs 3483\n",
      "22:16:55   line: 660000, found_pairs 17825, get_pairs 3664\n",
      "22:18:13   line: 690000, found_pairs 19139, get_pairs 3971\n",
      "22:19:43   line: 720000, found_pairs 19603, get_pairs 4072\n",
      "22:21:10   line: 750000, found_pairs 20120, get_pairs 4206\n",
      "22:22:35   line: 780000, found_pairs 21021, get_pairs 4411\n",
      "22:24:01   line: 810000, found_pairs 22604, get_pairs 4675\n",
      "22:25:25   line: 840000, found_pairs 24189, get_pairs 4941\n",
      "22:26:41   line: 870000, found_pairs 25110, get_pairs 5120\n",
      "22:27:54   line: 900000, found_pairs 25869, get_pairs 5271\n",
      "22:29:19   line: 930000, found_pairs 27552, get_pairs 5580\n",
      "22:30:43   line: 960000, found_pairs 28720, get_pairs 5804\n",
      "22:32:04   line: 990000, found_pairs 29942, get_pairs 6093\n",
      "22:33:30   line: 1020000, found_pairs 30809, get_pairs 6286\n",
      "22:34:56   line: 1050000, found_pairs 32187, get_pairs 6589\n",
      "22:36:10   line: 1080000, found_pairs 32498, get_pairs 6655\n",
      "22:37:24   line: 1110000, found_pairs 32685, get_pairs 6683\n",
      "22:38:36   line: 1140000, found_pairs 32922, get_pairs 6743\n",
      "22:39:52   line: 1170000, found_pairs 33442, get_pairs 6835\n",
      "22:41:11   line: 1200000, found_pairs 34953, get_pairs 7071\n",
      "22:42:30   line: 1230000, found_pairs 36562, get_pairs 7412\n",
      "22:43:47   line: 1260000, found_pairs 37943, get_pairs 7669\n",
      "22:44:55   line: 1290000, found_pairs 38365, get_pairs 7764\n",
      "22:46:05   line: 1320000, found_pairs 38620, get_pairs 7805\n",
      "22:47:31   line: 1350000, found_pairs 38693, get_pairs 7827\n",
      "22:48:52   line: 1380000, found_pairs 38846, get_pairs 7880\n",
      "22:49:59   line: 1410000, found_pairs 38912, get_pairs 7899\n",
      "22:51:38   line: 1440000, found_pairs 38998, get_pairs 7930\n",
      "22:52:59   line: 1470000, found_pairs 39108, get_pairs 7960\n",
      "22:54:36   line: 1500000, found_pairs 39231, get_pairs 8003\n",
      "22:55:43   line: 1530000, found_pairs 39264, get_pairs 8008\n",
      "22:56:49   line: 1560000, found_pairs 39315, get_pairs 8026\n",
      "22:58:16   line: 1590000, found_pairs 39403, get_pairs 8050\n",
      "22:59:37   line: 1620000, found_pairs 40958, get_pairs 8307\n",
      "23:00:58   line: 1650000, found_pairs 41615, get_pairs 8447\n",
      "23:02:19   line: 1680000, found_pairs 42683, get_pairs 8646\n",
      "23:03:38   line: 1710000, found_pairs 43975, get_pairs 8899\n",
      "23:05:00   line: 1740000, found_pairs 45228, get_pairs 9148\n"
     ]
    }
   ],
   "source": [
    "# O_array = []\n",
    "# E_array = []\n",
    "# Value_array = []\n",
    "\n",
    "# n_line = 0\n",
    "# cnt_found_pairs = 0\n",
    "# cnt_get_pairs = 0\n",
    "\n",
    "# vocab_nO = {}\n",
    "# nO_vocab = {}\n",
    "# with open('UMBC_tokenized.txt') as f_text:\n",
    "        \n",
    "#     while True:\n",
    "#         n_line += 1\n",
    "        \n",
    "# #         if cnt_get_pairs > 10:\n",
    "# #             break\n",
    "        \n",
    "#         line = f_text.readline()\n",
    "#         if len(line) == 0:\n",
    "#             break\n",
    "            \n",
    "#         O_E = hearst_patterns.find_hyponyms(line)\n",
    "#         if len(O_E) != 0:\n",
    "#             for (O, E) in O_E:\n",
    "#                 E = E.lower()\n",
    "#                 O = O.lower()\n",
    "#                 cnt_found_pairs += 1\n",
    "#                 if E in vocab_nE:\n",
    "#                     E_array.append(vocab_nE[E])\n",
    "                    \n",
    "#                     if O not in vocab_nO:\n",
    "#                         vocab_nO[O] = len(vocab_nO)\n",
    "#                         nO_vocab[len(vocab_nO) - 1] = O\n",
    "#                     O_array.append(vocab_nO[O])\n",
    "                    \n",
    "#                     Value_array.append(1)\n",
    "#                     cnt_get_pairs += 1\n",
    "        \n",
    "#         if n_line % 30000 == 0:\n",
    "#             logging.info(\"line: %d, found_pairs %d, get_pairs %d\" %(n_line, cnt_found_pairs, cnt_get_pairs))\n",
    "            \n",
    "#             with open('dumpEO', 'w') as dumpF:\n",
    "#                 matrix = sparse.coo_matrix((Value_array, (O_array, E_array))).tocsr()\n",
    "#                 dumpF.write(pickle.dumps((matrix, vocab_nO, nO_vocab)))\n",
    "# #                 dumpF.write(pickle.dumps(nO_vocab) + \"\\n\")\n",
    "# #                 dumpF.write(pickle.dumps(vocab_nO))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coo_Matrix = sparse.coo_matrix((Value_array, (O_array, E_array))).tocsr()\n",
    "# ac = coo_Matrix.tocoo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successful completion,  newfoundland 1\n",
      "information,  decision 1\n",
      "retroactive effect,  progress 1\n"
     ]
    }
   ],
   "source": [
    "# for v, O, E in zip(ac.data, ac.row, ac.col):\n",
    "#     print nO_vocab[O] + ', ', nE_vocab[E], v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dumpEO') as dumpF:\n",
    "    coo_Matrix, vocab_nO, nO_vocab = pickle.loads(dumpF.read())\n",
    "    ac = coo_Matrix.tocoo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.434666666667\n"
     ]
    }
   ],
   "source": [
    "sums = []\n",
    "with open(\"predict_Hearst\", \"w\") as predF:\n",
    "    groups = group_by_O(ac)\n",
    "\n",
    "    for O_gold in hipon_type[:,0]:\n",
    "        if O_gold in vocab_nO:\n",
    "            num_O_gold = vocab_nO[O_gold]\n",
    "            sums.append(np.array(groups[num_O_gold])[:,1].sum())\n",
    "            Es = np.array(sorted(groups[num_O_gold], key=itemgetter(1), reverse=True))[:,0]\n",
    "        else:\n",
    "            sums.append(0)\n",
    "            Es = np.array([])\n",
    "        \n",
    "        line = map(lambda x: nE_vocab[x], Es)\n",
    "        \n",
    "        predF.write(\"\\t\".join(line) + \"\\n\")\n",
    "        \n",
    "print np.array(sums).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR: 0.00505555555556\r\n",
      "MAP: 0.0017171345938\r\n",
      "P@1: 0.00333333333333\r\n",
      "P@3: 0.00233333333333\r\n",
      "P@5: 0.00178888888889\r\n",
      "P@15: 0.00119837199837\r\n"
     ]
    }
   ],
   "source": [
    "!python SemEval18-Task9/task9-scorer.py ./SemEval18-Task9/training/gold/1A.english.training.gold.txt ./predict_Hearst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def merge_dumpEO():\n",
    "    for file_name in os.listdir('./'):\n",
    "        if 'dumpEO' in file_name:\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PMI + SVD + SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat_word = re.compile('\\w+')\n",
    "def prepare(text):\n",
    "        try:\n",
    "            sentences = re.findall(pat_word, text)\n",
    "        except:\n",
    "            sentences = []\n",
    "        \n",
    "#         push = False\n",
    "#         for word in sentences:\n",
    "#             word = word.lower()\n",
    "#             if word in vocab_nE or word in hipons:\n",
    "#                 push = True\n",
    "#                 break\n",
    "                \n",
    "#         if push:\n",
    "#             return sentences\n",
    "#         else:\n",
    "#             return []\n",
    "\n",
    "        return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:22:20   line: 500000, words 0\n",
      "10:22:37   line: 1000000, words 0\n",
      "10:22:53   line: 1500000, words 0\n",
      "10:23:09   line: 2000000, words 0\n",
      "10:23:26   line: 2500000, words 0\n",
      "10:23:42   line: 3000000, words 0\n",
      "10:23:58   line: 3500000, words 0\n",
      "10:24:14   line: 4000000, words 0\n",
      "10:24:31   line: 4500000, words 0\n",
      "10:24:47   line: 5000000, words 0\n",
      "10:25:03   line: 5500000, words 0\n",
      "10:25:19   line: 6000000, words 0\n",
      "10:25:35   line: 6500000, words 0\n",
      "10:25:51   line: 7000000, words 0\n",
      "10:26:07   line: 7500000, words 0\n",
      "10:26:22   line: 8000000, words 0\n",
      "10:26:39   line: 8500000, words 0\n",
      "10:26:55   line: 9000000, words 0\n",
      "10:27:11   line: 9500000, words 0\n",
      "10:27:29   line: 10000000, words 0\n",
      "10:27:46   line: 10500000, words 0\n",
      "10:28:02   line: 11000000, words 0\n",
      "10:28:18   line: 11500000, words 0\n",
      "10:28:34   line: 12000000, words 0\n",
      "10:28:50   line: 12500000, words 0\n",
      "10:29:07   line: 13000000, words 0\n",
      "10:29:23   line: 13500000, words 0\n",
      "10:29:40   line: 14000000, words 0\n",
      "10:29:58   line: 14500000, words 0\n",
      "10:30:15   line: 15000000, words 0\n",
      "10:30:31   line: 15500000, words 0\n",
      "10:30:46   line: 16000000, words 0\n",
      "10:31:02   line: 16500000, words 0\n",
      "10:31:18   line: 17000000, words 0\n",
      "10:31:35   line: 17500000, words 0\n",
      "10:31:51   line: 18000000, words 0\n",
      "10:32:07   line: 18500000, words 0\n",
      "10:32:24   line: 19000000, words 0\n",
      "10:32:40   line: 19500000, words 0\n",
      "10:32:57   line: 20000000, words 0\n",
      "10:33:13   line: 20500000, words 0\n",
      "10:33:30   line: 21000000, words 0\n",
      "10:33:46   line: 21500000, words 0\n",
      "10:34:02   line: 22000000, words 0\n",
      "10:34:17   line: 22500000, words 0\n",
      "10:34:34   line: 23000000, words 0\n",
      "10:34:51   line: 23500000, words 0\n",
      "10:35:07   line: 24000000, words 0\n",
      "10:35:23   line: 24500000, words 0\n",
      "10:35:39   line: 25000000, words 0\n",
      "10:35:56   line: 25500000, words 0\n",
      "10:36:12   line: 26000000, words 0\n",
      "10:36:28   line: 26500000, words 0\n",
      "10:36:44   line: 27000000, words 0\n",
      "10:37:00   line: 27500000, words 0\n",
      "10:37:17   line: 28000000, words 0\n",
      "10:37:33   line: 28500000, words 0\n",
      "10:37:49   line: 29000000, words 0\n",
      "10:38:05   line: 29500000, words 0\n",
      "10:38:21   line: 30000000, words 0\n",
      "10:38:38   line: 30500000, words 0\n",
      "10:38:54   line: 31000000, words 0\n",
      "10:39:10   line: 31500000, words 0\n",
      "10:39:26   line: 32000000, words 0\n",
      "10:39:42   line: 32500000, words 0\n",
      "10:39:59   line: 33000000, words 0\n",
      "10:40:16   line: 33500000, words 0\n",
      "10:40:32   line: 34000000, words 0\n",
      "10:40:49   line: 34500000, words 0\n",
      "10:41:04   line: 35000000, words 0\n",
      "10:41:20   line: 35500000, words 0\n",
      "10:41:37   line: 36000000, words 0\n",
      "10:41:53   line: 36500000, words 0\n",
      "10:42:09   line: 37000000, words 0\n",
      "10:42:25   line: 37500000, words 0\n",
      "10:42:42   line: 38000000, words 0\n",
      "10:42:59   line: 38500000, words 0\n",
      "10:43:15   line: 39000000, words 0\n",
      "10:43:31   line: 39500000, words 0\n",
      "10:43:47   line: 40000000, words 0\n",
      "10:44:03   line: 40500000, words 0\n",
      "10:44:19   line: 41000000, words 0\n",
      "10:44:35   line: 41500000, words 0\n",
      "10:44:51   line: 42000000, words 0\n",
      "10:45:08   line: 42500000, words 0\n",
      "10:45:24   line: 43000000, words 0\n",
      "10:45:40   line: 43500000, words 0\n",
      "10:45:57   line: 44000000, words 0\n",
      "10:46:13   line: 44500000, words 0\n",
      "10:46:29   line: 45000000, words 0\n",
      "10:46:45   line: 45500000, words 0\n",
      "10:47:00   line: 46000000, words 0\n",
      "10:47:17   line: 46500000, words 0\n",
      "10:47:33   line: 47000000, words 0\n",
      "10:47:50   line: 47500000, words 0\n",
      "10:48:06   line: 48000000, words 0\n",
      "10:48:22   line: 48500000, words 0\n",
      "10:48:39   line: 49000000, words 0\n",
      "10:48:55   line: 49500000, words 0\n",
      "10:49:11   line: 50000000, words 0\n",
      "10:49:27   line: 50500000, words 0\n",
      "10:49:44   line: 51000000, words 0\n",
      "10:50:00   line: 51500000, words 0\n",
      "10:50:16   line: 52000000, words 0\n",
      "10:50:32   line: 52500000, words 0\n",
      "10:50:48   line: 53000000, words 0\n",
      "10:51:04   line: 53500000, words 0\n"
     ]
    }
   ],
   "source": [
    "Sent_array = []\n",
    "Word_array = []\n",
    "Value_array = []\n",
    "\n",
    "w_s_v = {}\n",
    "# common_array = []\n",
    "\n",
    "n_line = 1\n",
    "cnt_found_pairs = 0\n",
    "cnt_get_pairs = 0\n",
    "\n",
    "vocab_nW = {}\n",
    "nW_vocab = {}\n",
    "\n",
    "with open('UMBC_tokenized.txt') as f_text:\n",
    "        \n",
    "    while True:\n",
    "        \n",
    "#         if len(vocab_nW) > 1000:\n",
    "#             break\n",
    "        \n",
    "        line = f_text.readline()\n",
    "        if len(line) == 0:\n",
    "            break\n",
    "            \n",
    "        sent = prepare(line)\n",
    "        for word in sent:\n",
    "            word = word.lower()\n",
    "#             if word in vocab_nE or word in hipons:\n",
    "#                 push = True\n",
    "                \n",
    "            Sent_array.append(n_line)\n",
    "\n",
    "            if word not in vocab_nW:\n",
    "                vocab_nW[word] = len(vocab_nW)\n",
    "                nW_vocab[len(vocab_nW) - 1] = word\n",
    "            Word_array.append(vocab_nW[word])\n",
    "\n",
    "            Value_array.append(1)\n",
    "\n",
    "#             key = str(n_line) + '_' + str(vocab_nW[word])\n",
    "#             key = (n_line, vocab_nW[word])\n",
    "#             w_s_v.setdefault(key, 0)\n",
    "#             w_s_v[key] += 1\n",
    "\n",
    "        if n_line % 500000 == 0:\n",
    "            logging.info(\"line: %d, words %d\" %(n_line, len(vocab_nW)))\n",
    "            \n",
    "        if n_line % 2000000 == 0:\n",
    "            if len(Value_array) != 0:\n",
    "                with open('dumpPMI_total_' + str(n_line/2000000), 'w') as dumpF:\n",
    "                    matrix = sparse.coo_matrix((Value_array, (Word_array, Sent_array))).tocsr()\n",
    "                    dumpF.write(pickle.dumps(matrix))\n",
    "\n",
    "                with open('dumpWords_total_' + str(n_line/2000000), 'w') as dumpF:\n",
    "                    dumpF.write(pickle.dumps((vocab_nW, nW_vocab)))\n",
    "\n",
    "                Sent_array = []\n",
    "                Word_array = []\n",
    "                Value_array = []\n",
    "\n",
    "                vocab_nW = {}\n",
    "                nW_vocab = {}\n",
    "            \n",
    "        n_line += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dumpPMI', 'w') as dumpF:\n",
    "    matrix = sparse.coo_matrix((Value_array, (Word_array, Sent_array))).tocsr()\n",
    "    dumpF.write(pickle.dumps(matrix))\n",
    "with open('dumpWords', 'w') as dumpF:\n",
    "    dumpF.write(pickle.dumps((vocab_nW, nW_vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dumpPMI') as dumpF:\n",
    "    coo_Matrix = pickle.loads(dumpF.read())\n",
    "    ac = coo_Matrix.tocoo()\n",
    "    \n",
    "with open('dumpWords') as dumpF:\n",
    "    vocab_nW, nW_vocab = pickle.loads(dumpF.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sent_array = []\n",
    "Word_array = []\n",
    "Value_array = []\n",
    "\n",
    "for (w, s), v in w_s_v.items():\n",
    "    Sent_array.append(s)\n",
    "    Word_array.append(w)\n",
    "    Value_array.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = sparse.coo_matrix((Value_array, (Word_array, Sent_array))).tocsr()\n",
    "# for v, word, sent in zip(ac.data, ac.row, ac.col):\n",
    "#     print nW_vocab[word] + ', ', sent, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_sent_ij = {}\n",
    "word_i = {}\n",
    "sent_j = {}\n",
    "\n",
    "for v, word, sent in zip(ac.data, ac.row, ac.col):\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:32:04   'pattern' package found; tag filters are available for English\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from lightgbm import LGBMRanker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:23:19   loading projection weights from ./modelsW2V/GoogleNews-vectors-negative300.bin\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('./modelsW2V/GoogleNews-vectors-negative300.bin', binary=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "661 979\n",
      "0.67517875383\n"
     ]
    }
   ],
   "source": [
    "found = 0\n",
    "allO = 0\n",
    "for O, typeO in hipon_type:\n",
    "    if typeO == 'Concept':\n",
    "        allO += 1\n",
    "        try:\n",
    "            model.wv[O]\n",
    "            found += 1\n",
    "        except:\n",
    "            pass\n",
    "print found, allO\n",
    "print float(found) / allO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 30\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "found = 0\n",
    "allO = 0\n",
    "for O, typeO in hipon_test_type:\n",
    "    if typeO == 'Concept':\n",
    "        allO += 1\n",
    "        try:\n",
    "            model.wv[O]\n",
    "            found += 1\n",
    "        except:\n",
    "            pass\n",
    "print found, allO\n",
    "print float(found) / allO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff(v1, v2):\n",
    "    return (v1 - v2).reshape(1, len(v1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff(np.array([1,2,3]), np.array([4,5,6])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_DF_Bless():\n",
    "#     for O, typeO, Es in zip(hipon_type, hipers):\n",
    "#         if typeO == 'Concept':\n",
    "#             try:\n",
    "#                 vecO = model.wv[O]\n",
    "#                 for E in Es:\n",
    "#                     try:\n",
    "#                         vecE = model.wv[E]\n",
    "    X = []\n",
    "    y = []\n",
    "    cnt_rand, cnt_hyp = 0,0\n",
    "    with open('./sim-eval_datasets/datasets/bless.csv') as blessF:\n",
    "        for line in blessF.readlines():\n",
    "            parts = line.strip().split(';')\n",
    "            O = parts[0]\n",
    "            E = parts[1]\n",
    "            typeOE = parts[2]\n",
    "            \n",
    "            if typeOE == 'random' and cnt_hyp > cnt_rand:\n",
    "                try:\n",
    "                    O_vec = model.wv[O]\n",
    "                    E_vec = model.wv[E]\n",
    "                    X.append(diff(O_vec, E_vec))\n",
    "                    y.append(0)\n",
    "                    cnt_rand += 1\n",
    "                except:\n",
    "                    continue\n",
    "                    \n",
    "            if typeOE == 'hyper':\n",
    "                try:\n",
    "                    O_vec = model.wv[O]\n",
    "                    E_vec = model.wv[E]\n",
    "                    X.append(diff(O_vec, E_vec))\n",
    "                    y.append(1)\n",
    "                    cnt_hyp += 1\n",
    "                except:\n",
    "                    continue\n",
    "                    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(37)\n",
    "new_words_with_rand = {}\n",
    "new_words = {}\n",
    "def getXy(hipon_type, hipers):\n",
    "    X, y, g = [], [], []\n",
    "    for i, ((O, typeO), Es) in enumerate(zip(hipon_type, hipers)):\n",
    "        if typeO == 'Concept':\n",
    "            try:\n",
    "                vecO = model.wv[O]\n",
    "                new_words.setdefault(i, [])\n",
    "                new_words_with_rand.setdefault(i, [])\n",
    "                for pos, E in enumerate(Es):\n",
    "                    size_part = len(Es) / 3.0\n",
    "                    try:\n",
    "                        vecE = model.wv[E]\n",
    "                        X.append(diff(vecO, vecE))\n",
    "                        \n",
    "                        part = 3 - int(pos / size_part)\n",
    "                        y.append(part)\n",
    "                        \n",
    "                        new_words[i].append(E)\n",
    "                        new_words_with_rand[i].append(E)\n",
    "                    except:\n",
    "                        for rand_i in range(10):\n",
    "                            randWord = random.sample(words_model, k=1)[0]\n",
    "                            vecRand = model.wv[randWord]\n",
    "                            X.append(diff(vecO, vecRand))\n",
    "                            y.append(0)\n",
    "\n",
    "\n",
    "                            new_words_with_rand[i].append(randWord)\n",
    "                        \n",
    "                    g.append(i)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "    return X, y, g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_model = model.wv.vocab.keys()\n",
    "def create_DF():\n",
    "    X, y, g = getXy(hipon_type, hipers)\n",
    "    \n",
    "    X_train, X_test, y_train,  y_test, g_train, g_test = train_test_split(X, y, g, 0.66)\n",
    "    \n",
    "    return X_train, X_test, y_train,  y_test, g_train, g_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(37)\n",
    "def train_test_split(X, y, g, ratio=0.66):\n",
    "    g_train_set = random.sample(set(g), int(ratio * len(set(g))))\n",
    "    \n",
    "    X_train, X_test, y_train,  y_test, g_train, g_test = [], [], [], [], [], []\n",
    "    for x_i, y_i, g_i in zip(X, y, g):\n",
    "        if g_i in g_train_set:\n",
    "            X_train.append(x_i)\n",
    "            y_train.append(y_i)\n",
    "            g_train.append(g_i)\n",
    "        else:\n",
    "            X_test.append(x_i)\n",
    "            y_test.append(y_i)\n",
    "            g_test.append(g_i)\n",
    "\n",
    "    return X_train, X_test, y_train,  y_test, g_train, g_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_group(groups):\n",
    "    lens = []\n",
    "    \n",
    "    len_g = 1\n",
    "    g_prev = groups[0]\n",
    "    for g in groups[1:]:\n",
    "        if g == g_prev:\n",
    "            len_g += 1\n",
    "        else:\n",
    "            lens.append(len_g)\n",
    "            len_g = 1\n",
    "        g_prev = g\n",
    "    lens.append(len_g)\n",
    "    return lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train,  y_test, g_train, g_test = create_DF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1723 3481\n"
     ]
    }
   ],
   "source": [
    "print len(X_test), len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LGBM = LGBMRanker(learning_rate=0.1, n_estimators=100, subsample=0.8, max_depth=5, num_leaves = 64, verbose=1)\n",
    "model_LGBM = model_LGBM.fit(X_train, y_train, group=split_group(g_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_predicts(real, predict):\n",
    "    PF = 0\n",
    "    NT = 0\n",
    "    PT = 0\n",
    "    NF = 0\n",
    "\n",
    "    dif = []\n",
    "    for r, p in zip(real, predict):\n",
    "        if real == 0:\n",
    "            if predict == 0:\n",
    "                NT += 1\n",
    "            else:\n",
    "                PF += 1\n",
    "        else:\n",
    "            if predict == 0:\n",
    "                NF += 1\n",
    "            else:\n",
    "                PT += 1\n",
    "            dif.append(np.abs(r-p))\n",
    "    print float(NT) / len(NT + NF)\n",
    "    print dif.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_predict_gold(model, X_test, g_test):\n",
    "    predicts = model.predict(X_test)\n",
    "    with open('gold_Google', 'w') as goldF, \\\n",
    "         open('test_Google', 'w') as testF:\n",
    "            one_line = [predicts[0]]\n",
    "            g_last = g_test[0]\n",
    "            for g, p in zip(g_test[1:], predicts[1:]):\n",
    "                if g == g_last:\n",
    "                    one_line.append(p)\n",
    "                else:\n",
    "                    words = zip(*sorted(zip(one_line, new_words_with_rand[g_last]), key=itemgetter(0), reverse=True))[1]\n",
    "                    testF.write('\\t'.join(words) + '\\n')\n",
    "                \n",
    "                    goldF.write('\\t'.join(new_words[g_last]) + '\\n')\n",
    "                    \n",
    "                g_last = g\n",
    "\n",
    "            words = zip(*sorted(zip(one_line, new_words_with_rand[g_last]), key=itemgetter(0), reverse=True))[1]\n",
    "            testF.write('\\t'.join(words) + '\\n')\n",
    "            \n",
    "            goldF.write('\\t'.join(new_words[g_last]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR: 0.365590717924\r\n",
      "MAP: 0.313769351636\r\n",
      "P@1: 0.28\r\n",
      "P@3: 0.305185185185\r\n",
      "P@5: 0.313925925926\r\n",
      "P@15: 0.321268903936\r\n"
     ]
    }
   ],
   "source": [
    "write_predict_gold(model_LGBM, X_test, g_test)\n",
    "!python SemEval18-Task9/task9-scorer.py ./gold_Google ./test_Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR: 0.711407407407\r\n",
      "MAP: 0.677629479985\r\n",
      "P@1: 0.622222222222\r\n",
      "P@3: 0.653333333333\r\n",
      "P@5: 0.674074074074\r\n",
      "P@15: 0.696178599179\r\n"
     ]
    }
   ],
   "source": [
    "write_predict_gold(model_LGBM, X_test, g_test)\n",
    "!python SemEval18-Task9/task9-scorer.py ./gold_Google ./test_Google"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit собствнной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNsents(ffile, N=1000000):\n",
    "    sents = []\n",
    "    flag_EOF = False\n",
    "    for i in xrange(N):\n",
    "        sent = ffile.readline()\n",
    "        if len(sent) == 0:\n",
    "            flag_EOF = True\n",
    "            break\n",
    "            \n",
    "        words = prepare(sent)\n",
    "        if len(words) != 0:\n",
    "            sents.append(words)\n",
    "        \n",
    "    return sents, flag_EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(algorithm, window, size, min_count, alpha):\n",
    "    N = 1000000\n",
    "    with open('./UMBC_tokenized.txt') as file_text:\n",
    "        model = gensim.models.word2vec.Word2Vec(iter=5, sg=algorithm, window=window, size=size, min_count=min_count, alpha=alpha)\n",
    "\n",
    "        i = 0\n",
    "        while(True):\n",
    "            last_time = time()\n",
    "            sents, flag_EOF = getNsents(file_text, N)\n",
    "\n",
    "            if i == 0:\n",
    "                model.build_vocab(sents)\n",
    "            else:\n",
    "                model.build_vocab(sents, update=True)\n",
    "                model.train(sents, total_examples=self.corpus_count, epochs=self.iter)\n",
    "\n",
    "            i += N\n",
    "#             logging.info(\"Step: %d\" %i)\n",
    "            model.save('modelsW2V/my_model_' + str(i / N))\n",
    "\n",
    "            print \"Step: %d, Time: %d\" %(i, (time() - last_time) / 60)\n",
    "            if flag_EOF:\n",
    "                break\n",
    "                \n",
    "                        \n",
    "            \n",
    "                \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1000000, Time: 0\n"
     ]
    }
   ],
   "source": [
    "algorithm = 1 # Skip-gramm\n",
    "window = 7\n",
    "size = 300\n",
    "min_count = 1\n",
    "alpha = 0.025\n",
    "\n",
    "my_model = fit_model(algorithm, window, size, min_count, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "00:02:58   loading Word2Vec object from ./modelsW2V/my_model_18\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec.load(\"./modelsW2V/my_model_18\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
